{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 初次使用平台，您可请前往<a href=\"/public-project?tab=1\" target=\"_blank\">入门项目</a>了解项目使用规范和流程。\n",
    "##### For the first time using the platform, you can go to the Entry Project to understand the project usage specifications and procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-23T07:56:03.303856Z",
     "iopub.status.busy": "2022-04-23T07:56:03.303428Z",
     "iopub.status.idle": "2022-04-23T07:56:11.605340Z",
     "shell.execute_reply": "2022-04-23T07:56:11.604659Z",
     "shell.execute_reply.started": "2022-04-23T07:56:03.303767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out (2, 1, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "import megengine.functional as torch\n",
    "import megengine as meg\n",
    "import megengine.functional.nn as F\n",
    "import megengine.module as nn\n",
    "\n",
    "import numpy as np\n",
    "#import torch.nn as nn\n",
    "#import torch\n",
    "#import torch.nn.functional as F\n",
    "#from torch.utils.data import Dataset\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import os.path as osp\n",
    "import gc\n",
    "\n",
    "\n",
    "def default_conv(in_channels, out_channels, kernel_size, bias=True):\n",
    "    return nn.Conv2d(\n",
    "        in_channels, out_channels, kernel_size,\n",
    "        padding=(kernel_size // 2), bias=bias)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Sequential):\n",
    "    def __init__(\n",
    "            self, conv, in_channels, out_channels, kernel_size, stride=1, bias=False,\n",
    "            bn=True, act=nn.ReLU()):\n",
    "\n",
    "        m = [conv(in_channels, out_channels, kernel_size, bias=bias)]\n",
    "        if bn:\n",
    "            m.append(nn.BatchNorm2d(out_channels))\n",
    "        if act is not None:\n",
    "            m.append(act)\n",
    "\n",
    "        super(BasicBlock, self).__init__(*m)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, conv, n_feats, kernel_size,\n",
    "            bias=True, bn=False, act=nn.ReLU(), res_scale=1):\n",
    "\n",
    "        super(ResBlock, self).__init__()\n",
    "        m = []\n",
    "        for i in range(2):\n",
    "            m.append(conv(n_feats, n_feats, kernel_size, bias=bias))\n",
    "            if bn:\n",
    "                m.append(nn.BatchNorm2d(n_feats))\n",
    "            if i == 0:\n",
    "                m.append(act)\n",
    "\n",
    "        self.body = nn.Sequential(*m)\n",
    "        self.res_scale = res_scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x) * self.res_scale\n",
    "        res += x\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "class Upsampler(nn.Sequential):\n",
    "    def __init__(self, conv, scale, n_feats, bn=False, act=False, bias=True):\n",
    "\n",
    "        m = []\n",
    "        if (scale & (scale - 1)) == 0:  # Is scale = 2^n?\n",
    "            for _ in range(int(math.log(scale, 2))):\n",
    "                m.append(conv(n_feats, 4 * n_feats, 3, bias))\n",
    "                m.append(nn.PixelShuffle(2))\n",
    "                if bn:\n",
    "                    m.append(nn.BatchNorm2d(n_feats))\n",
    "                if act == 'relu':\n",
    "                    m.append(nn.ReLU())\n",
    "                elif act == 'prelu':\n",
    "                    m.append(nn.PReLU(n_feats))\n",
    "\n",
    "        elif scale == 3:\n",
    "            m.append(conv(n_feats, 9 * n_feats, 3, bias))\n",
    "            m.append(nn.PixelShuffle(3))\n",
    "            if bn:\n",
    "                m.append(nn.BatchNorm2d(n_feats))\n",
    "            if act == 'relu':\n",
    "                m.append(nn.ReLU())\n",
    "            elif act == 'prelu':\n",
    "                m.append(nn.PReLU(n_feats))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        super(Upsampler, self).__init__(*m)\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True,\n",
    "                 bn=False, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                              dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.reshape(x.shape[0], -1)\n",
    "\n",
    "\n",
    "class ChannelGate(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
    "        super(ChannelGate, self).__init__()\n",
    "        self.gate_channels = gate_channels\n",
    "        self.mlp = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
    "        )\n",
    "        self.pool_types = pool_types\n",
    "\n",
    "    def forward(self, x):\n",
    "        channel_att_sum = None\n",
    "        for pool_type in self.pool_types:\n",
    "            if pool_type == 'avg':\n",
    "                avg_pool = F.avg_pool2d(x, (x.shape[2], x.shape[3]), stride=(x.shape[2], x.shape[3]))\n",
    "                channel_att_raw = self.mlp(avg_pool)\n",
    "            elif pool_type == 'max':\n",
    "                max_pool = F.max_pool2d(x, (x.shape[2], x.shape[3]), stride=(x.shape[2], x.shape[3]))\n",
    "                channel_att_raw = self.mlp(max_pool)\n",
    "            elif pool_type == 'lp':\n",
    "                lp_pool = F.lp_pool2d(x, 2, (x.shape[2], x.shape[3]), stride=(x.shape[2], x.shape[3]))\n",
    "                channel_att_raw = self.mlp(lp_pool)\n",
    "            elif pool_type == 'lse':\n",
    "                # LSE pool only\n",
    "                lse_pool = logsumexp_2d(x)\n",
    "                channel_att_raw = self.mlp(lse_pool)\n",
    "\n",
    "            if channel_att_sum is None:\n",
    "                channel_att_sum = channel_att_raw\n",
    "            else:\n",
    "                channel_att_sum = channel_att_sum + channel_att_raw\n",
    "        scale = F.broadcast_to(F.expand_dims(F.sigmoid(channel_att_sum), (2, 3)), x.shape)\n",
    "        #scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "        return x * scale\n",
    "\n",
    "\n",
    "def logsumexp_2d(tensor):\n",
    "    tensor_flatten = tensor.view(tensor.shape[0], tensor.size(1), -1)\n",
    "    s, _ = torch.max(tensor_flatten, axis=2, keepdims=True)\n",
    "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
    "    return outputs\n",
    "\n",
    "\n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.concat((torch.expand_dims(torch.max(x, 1), 1), torch.expand_dims(torch.mean(x, 1), 1)), axis=1)\n",
    "        #return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "\n",
    "\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = 7\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size - 1) // 2, relu=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = F.sigmoid(x_out)  # broadcasting\n",
    "        return x * scale\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
    "        self.no_spatial = no_spatial\n",
    "        if not no_spatial:\n",
    "            self.SpatialGate = SpatialGate()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_out = self.ChannelGate(x)\n",
    "        if not self.no_spatial:\n",
    "            x_out = self.SpatialGate(x_out)\n",
    "        return x_out\n",
    "\n",
    "import numbers\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Layer Norm\n",
    "\n",
    "def to_3d(x):\n",
    "    b,c,h,w = x.shape\n",
    "    x = torch.transpose(x,(0,2,3,1))\n",
    "    x = x.reshape(b,h*w,c)\n",
    "    #return rearrange(x, 'b c h w -> b (h w) c')\n",
    "    return x\n",
    "\n",
    "def to_4d(x,h,w):\n",
    "    b,_,c = x.shape\n",
    "    x = torch.transpose(x,(0,2,1))  # b c (h w)\n",
    "    x = x.reshape(b,c,h,w)\n",
    "    #return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)\n",
    "    return x\n",
    "\n",
    "class BiasFree_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(BiasFree_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return x / torch.sqrt(sigma+1e-5) * self.weight\n",
    "\n",
    "class WithBias_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(WithBias_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(-1, keepdim=True)\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, LayerNorm_type):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        if LayerNorm_type =='BiasFree':\n",
    "            self.body = BiasFree_LayerNorm(dim)\n",
    "        else:\n",
    "            self.body = WithBias_LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        return to_4d(self.body(to_3d(x)), h, w)\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Gated-Dconv Feed-Forward Network (GDFN)\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        hidden_features = int(dim*ffn_expansion_factor)\n",
    "        self.hidden_features = hidden_features\n",
    "        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n",
    "\n",
    "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n",
    "\n",
    "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.project_in(x)\n",
    "        #x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
    "        x12 = self.dwconv(x)\n",
    "        #print('x12',x12.shape)\n",
    "        x1,x2 = x12[:,:self.hidden_features],x12[:,self.hidden_features:self.hidden_features*2]\n",
    "        #print(x1.shape,x2.shape)\n",
    "        x = F.gelu(x1) * x2\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Multi-DConv Head Transposed Self-Attention (MDTA)\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, bias):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        #self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "        self.temperature = meg.Parameter(torch.ones([num_heads, 1, 1]))\n",
    "        self.dim = dim\n",
    "        self.qkv = nn.Conv2d(dim, dim*3, kernel_size=1, bias=bias)\n",
    "        self.qkv_dwconv = nn.Conv2d(dim*3, dim*3, kernel_size=3, stride=1, padding=1, groups=dim*3, bias=bias)\n",
    "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b,c,h,w = x.shape\n",
    "\n",
    "        qkv = self.qkv_dwconv(self.qkv(x))\n",
    "        q,k,v = qkv[:,:self.dim,:,:],qkv[:,self.dim:self.dim*2,:,:],qkv[:,self.dim*2:self.dim*3,:,:]\n",
    "        #q,k,v = qkv.chunk(3,dim=1)\n",
    "        q = q.reshape(b,c,h*w).reshape(b,self.num_heads,c//self.num_heads,h*w)\n",
    "        k = k.reshape(b,c,h*w).reshape(b,self.num_heads,c//self.num_heads,h*w)\n",
    "        v = v.reshape(b,c,h*w).reshape(b,self.num_heads,c//self.num_heads,h*w)\n",
    "        #q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        #k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        #v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "\n",
    "        #q = torch.nn.functional.normalize(q, dim=-1)\n",
    "        #k = torch.nn.functional.normalize(k, dim=-1)\n",
    "        \n",
    "        q = torch.normalize(q, axis=-1)\n",
    "        k = torch.normalize(k, axis=-1)\n",
    "        \n",
    "        \n",
    "        #attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        attn = (q @ torch.transpose(k,(0,1,3,2))) * self.temperature\n",
    "        #attn = attn.softmax(dim=-1)\n",
    "        attn = F.softmax(attn,axis=-1)\n",
    "\n",
    "        out = (attn @ v)\n",
    "        \n",
    "        #out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
    "        out = out.reshape(b,c,h,w)\n",
    "\n",
    "        out = self.project_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, ffn_expansion_factor, bias, LayerNorm_type):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.norm1 =nn.Identity()#LayerNorm(dim, LayerNorm_type)\n",
    "        self.attn = Attention(dim, num_heads, bias)\n",
    "        self.norm2 =nn.Identity()#LayerNorm(dim, LayerNorm_type)\n",
    "        self.ffn = FeedForward(dim, ffn_expansion_factor, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Overlapped image patch embedding with 3x3 Conv\n",
    "class OverlapPatchEmbed(nn.Module):\n",
    "    def __init__(self, in_c=3, embed_dim=48, bias=False):\n",
    "        super(OverlapPatchEmbed, self).__init__()\n",
    "\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "## Resizing modules\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Downsample, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat//2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                  nn.PixelUnshuffle(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, n_feat):\n",
    "        super(Upsample, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                  nn.PixelShuffle(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "##########################################################################\n",
    "##---------- Restormer -----------------------\n",
    "\n",
    "def rand_bbox(size, cut_rat):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "def cutmix(input,target):\n",
    "    rand_index = torch.randperm(input.size()[0])\n",
    "    target_a = target\n",
    "    target_b = target[rand_index]\n",
    "    cut_ratio = np.random.uniform()\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(input.size(), cut_ratio)\n",
    "    input[:, :, bbx1:bbx2, bby1:bby2] = input[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "    target[:, :, bbx1:bbx2, bby1:bby2] = target[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "    # adjust lambda to exactly match pixel ratio\n",
    "   \n",
    "    return input,target\n",
    "class TCond(nn.Module):\n",
    "    def __init__(self, dim=32):\n",
    "        super().__init__()\n",
    "        self.t_embedding = nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "        self.x_embedding = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = self.t_embedding(t)\n",
    "        scale = self.x_embedding(x.mean((-1, -2)) + t) * 0.1\n",
    "        return scale\n",
    "class Restormer_lite(nn.Module):\n",
    "    def __init__(self, \n",
    "        inp_channels=4, \n",
    "        out_channels=4, \n",
    "        dim = 48,\n",
    "        num_blocks = [1,1,1], \n",
    "        num_refinement_blocks = 1,\n",
    "        heads = [2,8,8],\n",
    "        ffn_expansion_factor = 2.05,\n",
    "        num_recur = 4,\n",
    "        bias = False,\n",
    "        LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n",
    "        dual_pixel_task = False        ## True for dual-pixel defocus deblurring only. Also set inp_channels=6\n",
    "    ):\n",
    "\n",
    "        super(Restormer_lite, self).__init__()\n",
    "        self.num_recur = num_recur\n",
    "        self.patch_embed = OverlapPatchEmbed(inp_channels, dim)\n",
    "\n",
    "        self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "        \n",
    "        #self.down1_2 = Downsample(dim) ## From Level 1 to Level 2\n",
    "        #self.encoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
    "        \n",
    "        #self.up2_1 = Upsample(int(dim*2**1))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)\n",
    "        #self.reduce_chan_level1 = nn.Conv2d(int(dim*2**1), int(dim), kernel_size=1, bias=bias)\n",
    "        self.decoder_level1 = nn.Sequential(*[TransformerBlock(dim=int(dim), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "        \n",
    "        self.refinement = nn.Sequential(*[TransformerBlock(dim=int(dim), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_refinement_blocks)])\n",
    "        \n",
    "        self.attn_list = [\n",
    "            CBAM(\n",
    "                dim, reduction_ratio=2\n",
    "            ) for _ in range(self.num_recur)\n",
    "        ]\n",
    "            \n",
    "        self.output = nn.Conv2d(int(dim), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "        self.t_cond = TCond(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, c, h, w = x.shape\n",
    "        #x = x.reshape((n, c, h // 2, 2, w // 2, 2)).permute((0, 1, 3, 5, 2, 4))\n",
    "        x = torch.transpose(x.reshape((n, c, h // 2, 2, w // 2, 2))   ,(0, 1, 3, 5, 2, 4))\n",
    "        inp_img = x.reshape((n, c * 4, h // 2, w // 2))\n",
    "        inp_enc_level1 = self.patch_embed(inp_img)\n",
    "        \n",
    "        for i in range(self.num_recur):  \n",
    "            inp_enc_level1 = self.attn_list[i](inp_enc_level1)   \n",
    "            out_enc_level1 = self.encoder_level1(inp_enc_level1)\n",
    "        \n",
    "            #inp_enc_level2 = self.down1_2(out_enc_level1)\n",
    "            #out_enc_level2 = self.encoder_level2(inp_enc_level2)\n",
    "\n",
    "            #inp_dec_level1 = self.up2_1(out_enc_level2)\n",
    "            #inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)\n",
    "            #inp_dec_level1 = self.reduce_chan_level1(inp_dec_level1)\n",
    "            out_dec_level1 = self.decoder_level1(out_enc_level1)\n",
    "            out_dec_level1 = self.refinement(out_dec_level1)\n",
    "            t_cond_scale = self.t_cond(inp_enc_level1,torch.full((len(x),1),(i+1)/self.num_recur,device=inp_enc_level1.device))\n",
    "            #inp_enc_level1 = inp_enc_level1 + out_dec_level1 * (t_cond_scale.unsqueeze(2).unsqueeze(3)+1)\n",
    "            inp_enc_level1 = inp_enc_level1 + out_dec_level1 *( F.expand_dims(t_cond_scale,(2,3))  +1)\n",
    "            \n",
    "        \n",
    "        \n",
    "        out_dec_level1 = self.output(inp_enc_level1) + inp_img\n",
    "\n",
    "        #out_dec_level1 = out_dec_level1.reshape((n, c, 2, 2, h // 2, w // 2)).permute((0, 1, 4, 2, 5, 3))\n",
    "        out_dec_level1 = torch.transpose(out_dec_level1.reshape((n, c, 2, 2, h // 2, w // 2)), (0, 1, 4, 2, 5, 3))\n",
    "        out_dec_level1 = out_dec_level1.reshape((n, c, h, w))\n",
    "\n",
    "        return out_dec_level1\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    net = Restormer_lite()\n",
    "    #meg.save(net.state_dict(), \"dbg_cbam.pth\")\n",
    "\n",
    "    x = torch.zeros((2, 1, 256, 256))\n",
    "    out = net(x)\n",
    "    print('out',out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-23T07:56:11.607085Z",
     "iopub.status.busy": "2022-04-23T07:56:11.606772Z",
     "iopub.status.idle": "2022-04-23T07:56:11.613124Z",
     "shell.execute_reply": "2022-04-23T07:56:11.612541Z",
     "shell.execute_reply.started": "2022-04-23T07:56:11.607059Z"
    }
   },
   "outputs": [],
   "source": [
    "from megengine.data import DataLoader, RandomSampler,SequentialSampler\n",
    "from megengine.data.dataset import Dataset\n",
    "class DataFolder(Dataset):\n",
    "    def __init__(self, x,y):\n",
    "        self.x = x.reshape((-1,1,256,256))* np.float32(1 / 65536)\n",
    "        self.y = y.reshape((-1,1,256,256))* np.float32(1 / 65536)\n",
    "        n, c, h, w = self.x.shape\n",
    "\n",
    "        self.length = len(x)\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index],self.y[index]\n",
    " \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-23T07:56:11.614342Z",
     "iopub.status.busy": "2022-04-23T07:56:11.614069Z",
     "iopub.status.idle": "2022-04-23T07:56:23.253216Z",
     "shell.execute_reply": "2022-04-23T07:56:23.252593Z",
     "shell.execute_reply.started": "2022-04-23T07:56:11.614315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8192, 256, 256)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('loading data')\n",
    "content = open('dataset/burst_raw/competition_train_input.0.2.bin', 'rb').read()\n",
    "samples_ref = np.frombuffer(content, dtype = 'uint16').reshape((-1,256,256))\n",
    "content = open('dataset/burst_raw/competition_train_gt.0.2.bin', 'rb').read()\n",
    "samples_gt = np.frombuffer(content, dtype = 'uint16').reshape((-1,256,256))\n",
    "samples_ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-23T07:56:23.255166Z",
     "iopub.status.busy": "2022-04-23T07:56:23.254858Z",
     "iopub.status.idle": "2022-04-23T07:56:23.662227Z",
     "shell.execute_reply": "2022-04-23T07:56:23.661387Z",
     "shell.execute_reply.started": "2022-04-23T07:56:23.255134Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#_, X_test, _, y_test= train_test_split(samples_ref,samples_gt,test_size = 0.1,random_state=42)\n",
    "X_test, y_test = samples_ref[:100],samples_gt[:100]\n",
    "test_ds = DataFolder(X_test, y_test)\n",
    "#train_ds = DataFolder(samples_ref,samples_gt)\n",
    "#del samples_ref,content,samples_gt\n",
    "#gc.collect()\n",
    "batch_size = 4\n",
    "#train_dl = DataLoader(train_ds,batch_size=batch_size,shuffle=True)\n",
    "test_sampler = SequentialSampler(test_ds, batch_size=4, drop_last=False)\n",
    "test_dl = DataLoader(test_ds,test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-04-23T07:56:23.663748Z",
     "iopub.status.busy": "2022-04-23T07:56:23.663391Z",
     "iopub.status.idle": "2022-04-23T07:56:23.673113Z",
     "shell.execute_reply": "2022-04-23T07:56:23.672521Z",
     "shell.execute_reply.started": "2022-04-23T07:56:23.663694Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(samples_pred,samples_gt):\n",
    "    samples_pred = (samples_pred * 65536).clip(0,65535).astype(np.float32)\n",
    "    samples_gt = samples_gt.astype(np.float32)\n",
    "    means = samples_gt.mean(axis=(1, 2))\n",
    "    weight = (1/means)**0.5\n",
    "    diff = np.abs(samples_pred - samples_gt).mean(axis=(1, 2))\n",
    "    diff = diff * weight\n",
    "    score = diff.mean()\n",
    "    print('here score',score)\n",
    "    score = np.log10(100 / score) * 5\n",
    "    print('val score', score)\n",
    "    return score,samples_pred\n",
    "\n",
    "def evaluate_dl(model,test_dl,samples_gt):\n",
    "    model.eval()\n",
    "    samples_pred = []\n",
    "    for batch in tqdm(test_dl):\n",
    "        x, y = [meg.Tensor(d) for d in batch]\n",
    "        output = model(x)\n",
    "        samples_pred.append(output.numpy())\n",
    "        \n",
    "    \n",
    "    samples_pred = np.concatenate(samples_pred,axis=0).reshape(-1,256,256)\n",
    "    print(samples_pred.shape)\n",
    "    score,samples_pred = evaluate(samples_pred,samples_gt)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-23T07:56:23.674342Z",
     "iopub.status.busy": "2022-04-23T07:56:23.674030Z",
     "iopub.status.idle": "2022-04-23T07:56:23.711680Z",
     "shell.execute_reply": "2022-04-23T07:56:23.711115Z",
     "shell.execute_reply.started": "2022-04-23T07:56:23.674316Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import OrderedDict\n",
    "with open('weights.pkl','rb') as f:\n",
    "    weights = pickle.load(f)\n",
    "state_dicts = net.state_dict()\n",
    "for i in weights.keys():\n",
    "    weights[i] = weights[i].reshape( state_dicts[i].shape)\n",
    "net.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-23T07:56:23.712886Z",
     "iopub.status.busy": "2022-04-23T07:56:23.712644Z",
     "iopub.status.idle": "2022-04-23T07:56:26.634680Z",
     "shell.execute_reply": "2022-04-23T07:56:26.634178Z",
     "shell.execute_reply.started": "2022-04-23T07:56:23.712859Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:02<00:00,  9.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 256, 256)\n",
      "here score 1.6537352\n",
      "val score 8.907670198828221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.907670198828221"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_dl(net,test_dl,y_test)   #本地分数 线上分数为  8.99196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-23T07:56:26.635903Z",
     "iopub.status.busy": "2022-04-23T07:56:26.635607Z",
     "iopub.status.idle": "2022-04-23T07:56:52.885146Z",
     "shell.execute_reply": "2022-04-23T07:56:52.884533Z",
     "shell.execute_reply.started": "2022-04-23T07:56:26.635876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:25<00:00, 10.04it/s]\n"
     ]
    }
   ],
   "source": [
    "print('prediction')\n",
    "content = open('dataset/burst_raw/competition_test_input.0.2.bin', 'rb').read()\n",
    "samples_ref = np.frombuffer(content, dtype = 'uint16').reshape((-1,256,256))\n",
    "fout = open('workspace/result.bin', 'wb')\n",
    "batchsz = 4\n",
    "import tqdm\n",
    "\n",
    "for i in tqdm.tqdm(range(0, len(samples_ref), batchsz)):\n",
    "    i_end = min(i + batchsz, len(samples_ref))\n",
    "    batch_inp = meg.tensor(np.float32(samples_ref[i:i_end, None, :, :]) * np.float32(1 / 65536))\n",
    "    pred = net(batch_inp)\n",
    "    pred = (pred.numpy()[:, 0, :, :] * 65536).clip(0, 65535).astype('uint16')\n",
    "    fout.write(pred.tobytes())\n",
    "\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-23T07:56:52.886411Z",
     "iopub.status.busy": "2022-04-23T07:56:52.886148Z",
     "iopub.status.idle": "2022-04-23T07:56:52.894198Z",
     "shell.execute_reply": "2022-04-23T07:56:52.893633Z",
     "shell.execute_reply.started": "2022-04-23T07:56:52.886386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the real tranable parameter numbers is: 99674\n"
     ]
    }
   ],
   "source": [
    "params = sum([p.size for p in net.parameters()])\n",
    "print(f'the real tranable parameter numbers is:',params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2022-04-23T08:05:22.195404Z",
     "shell.execute_reply": "2022-04-23T08:05:22.194561Z",
     "shell.execute_reply.started": "2022-04-23T08:04:48.121375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:25<00:00,  9.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from predict import predict\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
